**HI-Labs Project-Based Training Plan (80 Hours) - Tabular Format**

---

# **PHASE 1: FOUNDATION BUILDING (28 HOURS)**
**Project Theme:** *Data Extraction & Analysis CLI Tool*

| Module            | Duration | Learning Objectives                                   | Detailed Topics                                                                                  | Project Tasks                                                                 |
|-------------------|----------|--------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| Linux Essentials  | 5 hrs    | Linux basics, commands, permissions, networking       | Distributions, commands, chmod/chown, ps/top, ping/curl/ifconfig                                | Script to organize CSVs, set permissions, test connectivity                    |
| SQL Basics        | 7 hrs    | SQL CRUD, joins, subqueries                           | SELECT, INSERT, UPDATE, DELETE, JOINS, AGGREGATES, SUBQUERIES                                   | Create "hospital" DB, import and query patient records                        |
| Python Basics     | 11 hrs   | Syntax, control flow, functions, file handling        | Variables, loops, functions, lists/dicts, file I/O, debugging                                    | Script to clean CSVs, validate, and save cleaned data                         |
| Case Study + Wrap | 2 hrs    | Combine Linux, SQL, Python skills                     | Integrated task: retrieve, clean, load, analyze                                                  | Complete CLI pipeline from Linux to Python to DB                              |

---

# **PHASE 2: INTERMEDIATE LEVEL (25 HOURS)**
**Project Theme:** *Automated Data Ingestion Pipeline*

| Module                 | Duration | Learning Objectives                                 | Detailed Topics                                                                                  | Project Tasks                                                                 |
|------------------------|----------|----------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| Advanced SQL           | 8 hrs    | Complex joins, CTEs, optimization                   | FULL JOIN, WINDOW FUNCTIONS, CTEs, INDEXES                                                       | Ranking report with window functions, optimize query plan                    |
| Python Advanced        | 6 hrs    | OOP, NumPy, Pandas, Visualization                   | Classes, binary files, NumPy, Pandas, Matplotlib                                                 | OOP-based data pipeline, visualize patient flows                              |
| AWS & Pipelines        | 7 hrs    | AWS S3/EC2, CI/CD, Airflow basics                  | boto3, CLI, pipeline triggers, DAG creation                                                      | Upload to S3, trigger Airflow DAG from Python                                 |
| Case Studies + Wrap-up | 4 hrs    | Real-world ingestion automation                     | EC2 file transfer, pipeline automation                                                           | SFTP → EC2 → S3 pipeline, Airflow trigger and notification                    |

---

# **PHASE 3: ADVANCED LEVEL (27 HOURS)**
**Project Theme:** *End-to-End Big Data Analytics Pipeline*

| Module                  | Duration | Learning Objectives                                 | Detailed Topics                                                                                  | Project Tasks                                                                 |
|-------------------------|----------|----------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|
| Big Data Technologies   | 9 hrs    | Hadoop, Snowflake, Databricks                      | HDFS, MapReduce, Snowflake DDL/DML, Data Integration                                             | Load data into Snowflake, compare analytics with Hadoop                        |
| Advanced Pipeline Dev   | 8 hrs    | DAGs, file ingestion pipelines                     | Airflow scheduling, dependencies, SFTP, automation                                               | Full ingestion to transformation pipeline with DAG & logging                  |
| Advanced Python Workflows | 9 hrs | AWS boto3, Dask, advanced logging, file ops       | Lambda triggers, monitoring, multi-format handling                                               | Process JSON/XML/Parquet, ETL logs to S3                                       |
| Capstone Project        | 4 hrs    | Integration of all tools in project                | Scope definition, build pipeline using Airflow, Pandas, Snowflake                               | SFTP → Python → Snowflake → DAG → Logs in S3 → Email report                   |

---

# **Final Assessment (3 HOURS)**

| Module         | Duration | Learning Objectives                           | Detailed Topics                                      | Project Tasks                                                                 |
|----------------|----------|----------------------------------------------|-----------------------------------------------------|---------------------------------------------------------------------------------|
| Final Review   | 3 hrs    | Validate complete learning journey            | Comprehensive quiz, project task                    | Build end-to-end pipeline with full tech stack                                 |

